{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PW Skills Assignment: Data Analytitcs: Logistic Regression**"
      ],
      "metadata": {
        "id": "VUEj4famq3yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Theoretical**"
      ],
      "metadata": {
        "id": "xWECWfdVrGuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Ans:\n",
        "* Logistic Regression is used for classification tasks.\n",
        "\n",
        "* Linear Regression is used for regression (predicting continuous values).\n",
        "\n",
        "* Logistic Regression outputs probabilities using a sigmoid function, while Linear Regression gives continuous outputs.\n",
        "\n",
        "Example: Predicting if an email is spam (1) or not (0)."
      ],
      "metadata": {
        "id": "jo9yCA-krOjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Ans: The **mathematical equation** of Logistic Regression is:\n",
        "\n",
        "$$\n",
        "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n)}}\n",
        "$$\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* It uses the **sigmoid function** to convert linear output into probability.\n",
        "* $\\beta_0$ is the intercept, and $\\beta_1, \\beta_2, ..., \\beta_n$ are coefficients.\n",
        "* Output is between **0 and 1**, interpreted as probability.\n"
      ],
      "metadata": {
        "id": "-Cz3M2eRrqFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Ans: The Sigmoid function maps any real number to a value between 0 and 1, making it suitable for probability outputs.\n",
        "\n",
        "**Example:** If the model output is 2, sigmoid(2) ≈ 0.88 (interpreted as 88% probability of class 1)."
      ],
      "metadata": {
        "id": "XUz7OMRRugf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression?\n",
        "\n",
        "Ans: The **cost function** of Logistic Regression is called **Log Loss** or **Binary Cross-Entropy**:\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "### Explanation (Pointwise):\n",
        "\n",
        "* $m$: Number of training examples\n",
        "* $y^{(i)}$: Actual label (0 or 1)\n",
        "* $h_\\theta(x^{(i)})$: Predicted probability from the sigmoid function\n",
        "* Measures **how far** predicted values are from the actual values.\n",
        "* Lower cost means **better model fit**.\n",
        "\n",
        "### Example:\n",
        "\n",
        "If the true label is 1 and predicted probability is 0.9, cost is low.\n",
        "If predicted probability is 0.1, cost is high.\n"
      ],
      "metadata": {
        "id": "H5VS5YAQu6v7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Ans: **Regularization** in Logistic Regression is a technique to **penalize large coefficients** in the model to **reduce overfitting**.\n",
        "\n",
        "\n",
        "\n",
        "### Why It’s Needed (Pointwise):\n",
        "\n",
        "* Prevents the model from learning **noise or irrelevant patterns**.\n",
        "* Encourages **simpler models** that generalize better on new data.\n",
        "* Helps when features are **correlated** or the dataset is **small**.\n",
        "\n",
        "\n",
        "\n",
        "### Types of Regularization:\n",
        "\n",
        "* **L1 (Lasso):** Shrinks some coefficients to zero (feature selection).\n",
        "* **L2 (Ridge):** Shrinks coefficients closer to zero but keeps all.\n",
        "* **Elastic Net:** Combines L1 and L2.\n",
        "\n",
        "\n",
        "### Example:\n",
        "\n",
        "If your model has many features with high weights, it may fit training data too well but perform poorly on test data.\n",
        "Regularization reduces those weights to make the model more **robust**.\n"
      ],
      "metadata": {
        "id": "Yzw2Yss8vVoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        " Ans: * Lasso (L1): Shrinks some coefficients to zero (feature selection).\n",
        "\n",
        "* Ridge (L2): Shrinks coefficients but none to zero.\n",
        "\n",
        "* Elastic Net: Combines L1 and L2.\n",
        "\n"
      ],
      "metadata": {
        "id": "ufvWxiPQv3D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Ans: Use Elastic Net when there are many correlated features; it combines L1 and L2 penalties for better performance."
      ],
      "metadata": {
        "id": "90CV8eyhwd4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Ans: λ (lambda) controls the strength of regularization.\n",
        "\n",
        "* High λ = more penalty, simpler model, less overfitting.\n",
        "\n",
        "* Low λ = less penalty, more complex model, risk of overfitting."
      ],
      "metadata": {
        "id": "Merd5VvKwn3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "Independent observations.\n",
        "\n",
        "Ans:\n",
        "* Binary dependent variable (for binary logistic regression).\n",
        "\n",
        "* Linearity between independent variables and log-odds.\n",
        "\n",
        "* No extreme outliers.\n",
        "\n",
        "* Large sample size."
      ],
      "metadata": {
        "id": "ot_ph_w7w4bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Ans:\n",
        "* Decision Trees\n",
        "\n",
        "* Random Forests\n",
        "\n",
        "* Support Vector Machines (SVM)\n",
        "\n",
        "* Naive Bayes\n",
        "\n",
        "* Neural Networks"
      ],
      "metadata": {
        "id": "APLRQ_MCxSjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n",
        "\n",
        "Ans:\n",
        "* Accuracy\n",
        "\n",
        "* Precision\n",
        "\n",
        "* Recall\n",
        "\n",
        "* F1-Score\n",
        "\n",
        "* ROC-AUC\n",
        "\n",
        "* Confusion Matrix"
      ],
      "metadata": {
        "id": "AkzoteUZxe5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "\n",
        "Ans: Class imbalance can cause the model to be biased towards the majority class, reducing performance on the minority class."
      ],
      "metadata": {
        "id": "LiBHmayHxvB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "\n",
        "Ans: Adjusting model parameters (like C, penalty) to find the best performance."
      ],
      "metadata": {
        "id": "p30N-OJTx2fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "\n",
        "\n",
        "Ans: In Logistic Regression, solvers are optimization algorithms used to find the best model parameters (coefficients).\n",
        "\n",
        "\n",
        "\n",
        "**Use:**\n",
        "\n",
        "* For small/binary problems: liblinear\n",
        "\n",
        "* For large datasets: lbfgs or saga\n",
        "\n",
        "* For L1 or ElasticNet regularization: saga only\n",
        "\n",
        "* For multiclass problems: lbfgs or saga"
      ],
      "metadata": {
        "id": "VqETwPSDx825"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "Ans: Using One-vs-Rest (OvR) or Softmax (multinomial) strategies."
      ],
      "metadata": {
        "id": "xdpfCEewy6Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "\n",
        "Ans: **Advantages**:\n",
        "\n",
        "* Simple, fast, interpretable.\n",
        "\n",
        "* Works well for linearly separable data.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Not suitable for complex relationships.\n",
        "\n",
        "* Assumes linearity in log-odds."
      ],
      "metadata": {
        "id": "kaB91w03zCB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?\n",
        "\n",
        "\n",
        "Ans:\n",
        "* Spam detection (email: spam/not spam)\n",
        "\n",
        "* Disease prediction (sick/not sick)\n",
        "\n",
        "* Customer churn prediction (churn/no churn)"
      ],
      "metadata": {
        "id": "zvDDBMxBzTym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "\n",
        "ans: **Softmax Regression**: Used for multiclass classification.\n",
        "\n",
        "**Logistic Regression**: Used for binary classification."
      ],
      "metadata": {
        "id": "tsNzcMT3zeBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "\n",
        "Ans: **OvR:** Simpler, works for most cases.\n",
        "\n",
        "\n",
        "\n",
        "**Softmax:** Preferred for true multiclass problems with mutual exclusivity."
      ],
      "metadata": {
        "id": "I-SpQkW9zqXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans:\n",
        "* Positive → increases log-odds\n",
        "\n",
        "* Negative → decreases log-odds\n",
        "\n",
        "** Use exp(coef_) to interpret as odds ratio"
      ],
      "metadata": {
        "id": "MBktxwdYz2gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Practical**"
      ],
      "metadata": {
        "id": "ku5uxXF-0QUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z2EcrL80ZBj",
        "outputId": "463801a0-06f7-4694-def0-c7ac4469c33d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"L1 Regularization Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAtPuuKz0wct",
        "outputId": "28ba43fb-6d96-4631-8afb-005ca9c08c3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Regularization Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"L2 Regularization Accuracy:\", model.score(X_test, y_test))\n",
        "print(\"Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpxJBwxO0-ho",
        "outputId": "9796e908-82f8-43e0-ac93-375a82f649ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Regularization Accuracy: 1.0\n",
            "Coefficients: [[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Elastic Net Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoY-UBOx1JrV",
        "outputId": "295f6c72-9e11-4568-9342-993386dc08d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OvR Multiclass Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHzhsnvQ1WSf",
        "outputId": "94efd93b-8f65-4361-e4a5-28b4389c066f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvR Multiclass Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnGK--Tj1h8P",
        "outputId": "2afcc0e7-fd46-4c5f-fc33-90ef610532e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best Accuracy: 0.9500000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=skf)\n",
        "print(\"Average Stratified K-Fold Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_cL0g3W1tyA",
        "outputId": "83642f94-8047-4aef-d75c-b20024082639"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Stratified K-Fold Accuracy: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('your_file.csv')\n",
        "# X = df.drop('target', axis=1)\n",
        "# y = df['target']\n",
        "# For demonstration, using iris data\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"CSV Data Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh11Ka-d15Ct",
        "outputId": "00af1e34-d111-4845-bf87-90ec14b00d20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Data Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_distributions=param_dist, n_iter=5, cv=3)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Params:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5iLRiQ82CkT",
        "outputId": "2cb7ca4d-bfdc-446c-c6fe-b70ae5395103"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'solver': 'saga', 'penalty': 'l1', 'C': 1}\n",
            "Best Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "ovo = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo.fit(X_train, y_train)\n",
        "print(\"OvO Multiclass Accuracy:\", ovo.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi7QpM3h2QLq",
        "outputId": "1788cc12-72a8-4e4d-f606-6bde8736fe1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Multiclass Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "tHYJf8a62ild",
        "outputId": "7fd39d47-4108-4864-fba7-02693f615c22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3JYfP5w2reU",
        "outputId": "937e3995-c675-4758-f7bf-4e0844950c73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X_imb, y_imb = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imb, y_imb, test_size=0.2)\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Imbalanced Data Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQl8Heqb26MZ",
        "outputId": "97b7e19a-b277-440f-d282-9d2ac9690190"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced Data Accuracy: 0.865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "titanic = sns.load_dataset('titanic').dropna(subset=['age', 'fare', 'embarked', 'sex', 'pclass', 'survived'])\n",
        "X = titanic[['age', 'fare', 'pclass']]\n",
        "y = titanic['survived']\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Titanic Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76P1njNf3EvV",
        "outputId": "57471efb-13bb-484e-c2da-eb5af462660a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titanic Accuracy: 0.7062937062937062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "acc_unscaled = model.score(X_test, y_test)\n",
        "model.fit(X_scaled[:len(X_train)], y_train)\n",
        "acc_scaled = model.score(X_scaled[len(X_train):], y_test)\n",
        "print(\"Without Scaling:\", acc_unscaled)\n",
        "print(\"With Scaling:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1rCM3cW3N2I",
        "outputId": "af75b51b-220a-4c37-95cb-b59afb742a20"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Scaling: 0.7062937062937062\n",
            "With Scaling: 0.6013986013986014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRKsNJ1v3etD",
        "outputId": "ce1c6814-1996-44ec-dd6e-e1e901004d1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.38892288861689106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Custom C=0.5 Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULwwAw8k3pB5",
        "outputId": "881731a1-9a8d-40d3-b396-7722cbe28214"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom C=0.5 Accuracy: 0.7062937062937062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", np.abs(model.coef_).sum(axis=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-ThUMg03yDk",
        "outputId": "048dbd32-29ad-4ea5-d567-bcb1fa46a546"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.04256608 0.00300163 1.15200426]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Cohen's Kappa Score:\", cohen_kappa_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgP_VGr637iC",
        "outputId": "cdcc9d7f-f352-42fa-9225-dce0c1a153c3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.3687197813748161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
        "disp = PrecisionRecallDisplay(precision=prec, recall=rec)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "6_Sn6tBy4FCd",
        "outputId": "8ffda9fb-6737-462d-f701-59902ad9699b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Solver: {solver}, Accuracy: {model.score(X_test, y_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XEAIn6F4RQb",
        "outputId": "37dc4206-a061-488d-8c66-74305af42261"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 0.6993006993006993\n",
            "Solver: saga, Accuracy: 0.6153846153846154\n",
            "Solver: lbfgs, Accuracy: 0.7062937062937062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Matthews Correlation Coefficient:\", matthews_corrcoef(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cSyPWG_4a1u",
        "outputId": "165c1fd6-8690-4775-b7af-c654a301d1a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.37295667992256104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Logistic Regression on Raw Data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy on Raw Data:\", acc_raw)\n",
        "\n",
        "# 4. Standardize the Data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 5. Logistic Regression on Scaled Data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy on Standardized Data:\", acc_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmJi6m5n4kUw",
        "outputId": "53b81d8f-b5e0-4b82-895c-781b6101dbbd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 0.956140350877193\n",
            "Accuracy on Standardized Data: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Define parameter grid for C\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# 5. GridSearchCV with cross-validation (cv=5)\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 6. Best C and corresponding accuracy\n",
        "print(\"Best C value:\", grid.best_params_['C'])\n",
        "print(\"Best cross-validation accuracy:\", grid.best_score_)\n",
        "\n",
        "# 7. Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "test_accuracy = best_model.score(X_test_scaled, y_test)\n",
        "print(\"Test set accuracy with best C:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZH47mn147xk",
        "outputId": "16bebf37-70d1-43f0-d67f-d6d5e54f105f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value: 10\n",
            "Best cross-validation accuracy: 0.9758241758241759\n",
            "Test set accuracy with best C: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # for saving and loading models\n",
        "import os\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Save the trained model\n",
        "model_filename = \"logistic_model.joblib\"\n",
        "joblib.dump(model, model_filename)\n",
        "print(\"Model saved as:\", model_filename)\n",
        "\n",
        "# 5. Load the saved model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "# 6. Make predictions and evaluate\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of loaded model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n52xC4zs5X30",
        "outputId": "6f9d8058-9bca-42ef-edbb-03cf9750cf42"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as: logistic_model.joblib\n",
            "Accuracy of loaded model: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}